#!/usr/bin/env python
# encoding: utf-8
import sys
import os
import inspect

current_dir = os.path.realpath(os.path.abspath(os.path.dirname(inspect.getfile(inspect.currentframe()))))

## for when config is still in recipes directory:
sys.path.append(current_dir + '/../scripts/')
sys.path.append(current_dir + '/../scripts/processors/')

## for after config is copied to voice.cfg:
sys.path.append(current_dir + '/../../../../scripts/')
sys.path.append(current_dir + '/../../../../scripts/processors/')

from Tokenisers import RegexTokeniser
from Phonetisers import NaivePhonetiser
from VSMTagger import VSMTagger
from FeatureExtractor import WorldExtractor
from FeatureDumper import FeatureDumper
from Aligner import StateAligner
from SKLProcessors import SKLDecisionTreePausePredictor
from PhraseMaker import PhraseMaker
from AcousticModel import AcousticModelWorld
from NN import NNDurationPredictor, NNAcousticPredictor

import default.const as c

## ----------------------------------------------------------------
## First define a few things used later:

## Some useful Xpaths and regex:--
# CONTENT_NODES = "//token[@token_class='word'] | //token[@token_class='punctuation']"
JUNCTURE_NODES = "//token[@token_class='space'] | //token[@token_class='punctuation']"

LETTER_PATT = '[\p{L}||\p{N}||\p{M}]'
CN_PUNC = "||".join(list(u"！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝·～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏."))
PUNC_OR_SPACE_PATT = '[\p{Z}||\p{C}||\p{P}||\p{S}||%s]' % CN_PUNC
PUNC_PATT = '[\p{C}||\p{P}||\p{S}||%s]' % CN_PUNC
SPACE_PATT = '\p{Z}'

tokenisation_pattern = '(' + SPACE_PATT + '*' + PUNC_PATT + '*' + SPACE_PATT + '+|' + SPACE_PATT + '*' + PUNC_PATT + '+\Z)'
## modified to handle word-internal hyphens

word_vsm_dim = 10

speech_coding_config = {'order': 59, 'static_window': '1', 'delta_window': '-0.5 0.0 0.5',
                        'delta_delta_window': '1.0 -2.0 1.0'}

## Define various context features used by processors:

pause_predictor_features = [
    ('response', './attribute::has_silence="yes"'),
    ('token_is_punctuation', './attribute::token_class="punctuation"'),
    ('since_start_utterance_in_words', "count(preceding::token[@token_class='word'])"),
    ('till_end_utterance_in_words', "count(following::token[@token_class='word'])")
]
## add word VSM features to pause predictor:
for dim in range(1, word_vsm_dim + 1):
    pause_predictor_features.append(
        ('L_vsm_d%s' % (dim), "./preceding::token[@token_class='word'][1]/attribute::vsm_d%s" % (dim)))
    pause_predictor_features.append(('C_vsm_d%s' % (dim), "./attribute::vsm_d%s" % (dim)))
    pause_predictor_features.append(
        ('R_vsm_d%s' % (dim), "./following::token[@token_class='word'][1]/attribute::vsm_d%s" % (dim)))

## These can be used from either the state or phone level:
phone_and_state_contexts = [
    ## WORD LEVEL:
    ("length_left_word", "count(ancestor::token/preceding::token[@token_class='word'][1]/descendant::segment)"),
    ("length_current_word", "count(ancestor::token/descendant::segment)"),
    ("length_right_word", "count(ancestor::token/following::token[@token_class='word'][1]/descendant::segment)"),
    ("since_beginning_of_word", "count_Xs_since_start_Y('segment', 'token')"),
    ("till_end_of_word", "count_Xs_till_end_Y('segment', 'token')"),

    ## phrase LEVEL:
    ("length_l_phrase_in_words", "count(ancestor::phrase/preceding::phrase[1]/descendant::token[@token_class='word'])"),
    ("length_c_phrase_in_words", "count(ancestor::phrase/descendant::token[@token_class='word'])"),
    ("length_r_phrase_in_words", "count(ancestor::phrase/following::phrase[1]/descendant::token[@token_class='word'])"),

    ("length_l_phrase_in_segments", "count(ancestor::phrase/preceding::phrase[1]/descendant::segment)"),
    ("length_c_phrase_in_segments", "count(ancestor::phrase/descendant::segment)"),
    ("length_r_phrase_in_segments", "count(ancestor::phrase/following::phrase[1]/descendant::segment)"),

    ("since_phrase_start_in_segs", "count_Xs_since_start_Y('segment', 'phrase')"),
    ("till_phrase_end_in_segs", "count_Xs_till_end_Y('segment', 'phrase')"),

    ("since_phrase_start_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'phrase')"),
    ("till_phrase_end_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'phrase')"),

    ## SENTENCE (UTT) LEVEL
    ("since_start_sentence_in_segments", "count_Xs_since_start_Y('segment', 'utt')"),
    ("since_start_sentence_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'utt')"),
    ("since_start_sentence_in_phrases", "count_Xs_since_start_Y('phrase', 'utt')"),
    ("till_end_sentence_in_segments", "count_Xs_till_end_Y('segment', 'utt')"),
    ("till_end_sentence_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'utt')"),
    ("till_end_sentence_in_phrases", "count_Xs_till_end_Y('phrase', 'utt')"),
    ("length_sentence_in_segments", "count(ancestor::utt/descendant::segment)"),
    ("length_sentence_in_words", "count(ancestor::utt/descendant::token[@token_class='word'])"),
    ("length_sentence_in_phrases", "count(ancestor::utt/descendant::phrase)")
]

## add word VSM features:
for dim in range(1, word_vsm_dim + 1):
    phone_and_state_contexts.append(
        ('L_vsm_d%s' % (dim), "./ancestor::token/preceding::token[@token_class='word'][1]/attribute::vsm_d%s" % (dim)))
    phone_and_state_contexts.append(('C_vsm_d%s' % (dim), "./ancestor::token/attribute::vsm_d%s" % (dim)))
    phone_and_state_contexts.append(
        ('R_vsm_d%s' % (dim), "./ancestor::token/following::token[@token_class='word'][1]/attribute::vsm_d%s" % (dim)))

phone_contexts = [
                     ## special named features:
                     ('htk_monophone', './attribute::pronunciation'),
                     ('start_time', './attribute::start'),
                     ('end_time', './attribute::end'),

                     ## normal features:
                     ('ll_segment', 'preceding::segment[2]/attribute::pronunciation'),
                     ('l_segment', 'preceding::segment[1]/attribute::pronunciation'),
                     ('c_segment', './attribute::pronunciation'),
                     ('r_segment', 'following::segment[1]/attribute::pronunciation'),
                     ('rr_segment', 'following::segment[2]/attribute::pronunciation')
                 ] + phone_and_state_contexts

state_contexts = [
                     ## special named features:
                     ("start_time", "./attribute::start"),
                     ("end_time", "./attribute::end"),
                     ("htk_state", "count(./preceding-sibling::state) + 1"),
                     ("htk_monophone", "./ancestor::segment/attribute::pronunciation"),

                     ("ll_segment", "./ancestor::segment/preceding::segment[2]/attribute::pronunciation"),
                     ("l_segment", "./ancestor::segment/preceding::segment[1]/attribute::pronunciation"),
                     ("c_segment", "./ancestor::segment/attribute::pronunciation"),
                     ("r_segment", "./ancestor::segment/following::segment[1]/attribute::pronunciation"),
                     ("rr_segment", "./ancestor::segment/following::segment[2]/attribute::pronunciation")
                 ] + phone_and_state_contexts

## This is a set of Xpaths for obtaining state durations:-
duration_data_contexts = [
    ('state_%s_nframes' % (i), '(./state[%s]/attribute::end - ./state[%s]/attribute::start) div 5' % (i, i)) for i in
    [1, 2, 3, 4, 5]]

## ----------------------------------------------------------------
## Now, a number of utterance processors are defined:--

tokeniser = RegexTokeniser('word_splitter', target_nodes='//utt', split_attribute='text', \
                           child_node_type='token', add_terminal_tokens=True, \
                           class_patterns=[('space', '\A' + SPACE_PATT + '+\Z'),
                                           ('punctuation', '\A' + PUNC_OR_SPACE_PATT + '+\Z')], \
                           split_pattern=tokenisation_pattern)

phonetiser = NaivePhonetiser('segment_adder', target_nodes="//token", target_attribute='text',
                             child_node_type='segment', \
                             class_attribute='token_class', output_attribute='pronunciation', word_classes=['word'], \
                             probable_pause_classes=['punctuation', c.TERMINAL], possible_pause_classes=['space'],
                             use_pinyin=True)

word_vector_tagger = VSMTagger('word_vector_tagger', target_nodes="//token[@token_class='word']",
                               input_attribute='text', \
                               output_attribute_stem='vsm', norm_counts=True, svd_algorithm='randomized',
                               context_size=250, rank=word_vsm_dim, \
                               unseen_method=1, discretisation_method='none', tokenisation_pattern=tokenisation_pattern)

speech_feature_extractor = WorldExtractor('acoustic_feature_extractor', input_filetype='wav', output_filetype='cmp', \
                                          coding_config=speech_coding_config, sample_rate=16000, alpha=0.77,
                                          mcep_order=59)

align_label_dumper = FeatureDumper(target_nodes='//segment', output_filetype='align_lab',
                                   contexts=[('segment', './attribute::pronunciation')])

aligner = StateAligner(target_nodes='//segment', target_attribute='pronunciation', input_label_filetype='align_lab',
                       acoustic_feature_filetype='cmp', \
                       output_label_filetype='time_lab', silence_tag='has_silence', min_silence_duration=50,
                       viterbi_beam_width='1000 100000 1000000', \
                       acoustic_subrecipe='standard_alignment')

pause_predictor = SKLDecisionTreePausePredictor(processor_name='pause_predictor', target_nodes=JUNCTURE_NODES,
                                                output_attribute='silence_predicted', contexts=pause_predictor_features)

phrase_adder = PhraseMaker(node_type_to_regroup='token', parent_node_type='phrase', \
                           attribute_with_silence='pronunciation', silence_symbol='sil')

dur_data_maker = FeatureDumper(processor_name='duration_data_maker', target_nodes='//segment',
                               context_separators='spaces', output_filetype='dur', \
                               question_file='null', contexts=duration_data_contexts, binary_output=True)

dur_label_maker = FeatureDumper(processor_name='labelmaker', target_nodes='//segment', context_separators='numbers',
                                output_filetype='lab_dur', \
                                question_file='questions_dur.hed', question_filter_threshold=5, contexts=phone_contexts)

duration_predictor = NNDurationPredictor(processor_name='duration_predictor', question_file='questions_dur.hed',
                                         target_nodes='//segment')

dnn_label_maker = FeatureDumper(processor_name='dnn_labelmaker', target_nodes='//state', context_separators='numbers',
                                output_filetype='lab_dnn', \
                                question_file='questions_dnn.hed', question_filter_threshold=5, contexts=state_contexts)

acoustic_predictor = NNAcousticPredictor(variance_expansion=0.3, sample_rate=16000, alpha=0.77, mcep_order=59,
                                         input_label_filetype='lab_dnn')

## -----------------------------------------------------------------
## The processors are grouped for convenience into several 'stages':

text_proc = [tokeniser, phonetiser, word_vector_tagger]
alignment = [align_label_dumper, speech_feature_extractor, aligner, pause_predictor, phrase_adder, dur_data_maker]
pause_prediction = [pause_predictor, phrase_adder]
speech_generation = [dur_label_maker, duration_predictor, dnn_label_maker, acoustic_predictor]

## ----------------------------------------------------------------
## The final part of the config specifies which stages are run in each of the modes
## "train" and "runtime" (and optionally extra, specialised, modes):

train_stages = [text_proc, alignment, speech_generation]

runtime_stages = [text_proc, pause_prediction, speech_generation]
